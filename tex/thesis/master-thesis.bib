@misc{registers,
  title         = {Vision Transformers Need Registers},
  author        = {Timothée Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski},
  year          = {2024},
  eprint        = {2309.16588},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2309.16588}
}
@misc{mamba-needs-registers,
  title         = {Mamba-R: Vision Mamba ALSO Needs Registers},
  author        = {Feng Wang and Jiahao Wang and Sucheng Ren and Guoyizhe Wei and Jieru Mei and Wei Shao and Yuyin Zhou and Alan Yuille and Cihang Xie},
  year          = {2024},
  eprint        = {2405.14858},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2405.14858}
}

@article{vit-survey,
  author     = {Khan, Salman and Naseer, Muzammal and Hayat, Munawar and Zamir, Syed Waqas and Khan, Fahad Shahbaz and Shah, Mubarak},
  title      = {Transformers in Vision: A Survey},
  year       = {2022},
  issue_date = {January 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {54},
  number     = {10s},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3505244},
  doi        = {10.1145/3505244},
  abstract   = {Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks, e.g., Long short-term memory. Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text, and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers, i.e., self-attention, large-scale pre-training, and bidirectional feature encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization), and three-dimensional analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works. We hope this effort will ignite further interest in the community to solve current challenges toward the application of transformer models in computer vision.},
  journal    = {ACM Comput. Surv.},
  month      = sep,
  articleno  = {200},
  numpages   = {41},
  keywords   = {Self-attention, transformers, bidirectional encoders, deep neural networks, convolutional networks, self-supervision, literature survey}
}

@inproceedings{deit3,
  author    = {Touvron, Hugo
               and Cord, Matthieu
               and J{\'e}gou, Herv{\'e}},
  editor    = {Avidan, Shai
               and Brostow, Gabriel
               and Ciss{\'e}, Moustapha
               and Farinella, Giovanni Maria
               and Hassner, Tal},
  title     = {DeiT III: Revenge of the ViT},
  booktitle = {Computer Vision -- ECCV 2022},
  year      = {2022},
  publisher = {Springer Nature Switzerland},
  address   = {Cham},
  pages     = {516--533},
  abstract  = {A Vision Transformer (ViT) is a simple neural architecture amenable to serve several computer vision tasks. It has limited built-in architectural priors, in contrast to more recent architectures that incorporate priors either about the input data or of specific tasks. Recent works show that ViTs benefit from self-supervised pre-training, in particular BerT-like pre-training like BeiT.},
  isbn      = {978-3-031-20053-3}
}

@misc{vit-state-challenges,
  title         = {Vision Transformers: State of the Art and Research Challenges},
  author        = {Bo-Kai Ruan and Hong-Han Shuai and Wen-Huang Cheng},
  year          = {2022},
  eprint        = {2207.03041},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2207.03041}
}

@misc{visiontransformers2021,
  title         = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author        = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  year          = {2021},
  eprint        = {2010.11929},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2010.11929}
}

@misc{transformer2017,
  title         = {Attention Is All You Need},
  author        = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  year          = {2023},
  eprint        = {1706.03762},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1706.03762}
}

@misc{dino,
  title         = {Emerging Properties in Self-Supervised Vision Transformers},
  author        = {Mathilde Caron and Hugo Touvron and Ishan Misra and Hervé Jégou and Julien Mairal and Piotr Bojanowski and Armand Joulin},
  year          = {2021},
  eprint        = {2104.14294},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2104.14294}
}

@misc{dinov2,
  title         = {DINOv2: Learning Robust Visual Features without Supervision},
  author        = {Maxime Oquab and Timothée Darcet and Théo Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Hervé Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
  year          = {2024},
  eprint        = {2304.07193},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2304.07193}
}

@misc{lost,
  title         = {Localizing Objects with Self-Supervised Transformers and no Labels},
  author        = {Oriane Siméoni and Gilles Puy and Huy V. Vo and Simon Roburin and Spyros Gidaris and Andrei Bursuc and Patrick Pérez and Renaud Marlet and Jean Ponce},
  year          = {2021},
  eprint        = {2109.14279},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2109.14279}
}

@misc{deit,
  title         = {Training data-efficient image transformers \& distillation through attention},
  author        = {Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Hervé Jégou},
  year          = {2021},
  eprint        = {2012.12877},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2012.12877}
}

@misc{deepvit,
  title         = {DeepViT: Towards Deeper Vision Transformer},
  author        = {Daquan Zhou and Bingyi Kang and Xiaojie Jin and Linjie Yang and Xiaochen Lian and Zihang Jiang and Qibin Hou and Jiashi Feng},
  year          = {2021},
  eprint        = {2103.11886},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2103.11886}
}

@inproceedings{open-clip,
  title     = {Reproducible Scaling Laws for Contrastive Language-Image Learning},
  url       = {http://dx.doi.org/10.1109/CVPR52729.2023.00276},
  doi       = {10.1109/cvpr52729.2023.00276},
  booktitle = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher = {IEEE},
  author    = {Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
  year      = {2023},
  month     = jun,
  pages     = {2818–2829}
}

@misc{memorytransformer,
  title         = {Memory Transformer},
  author        = {Mikhail S. Burtsev and Yuri Kuratov and Anton Peganov and Grigory V. Sapunov},
  year          = {2021},
  eprint        = {2006.11527},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/2006.11527}
}

@misc{vision-mamba,
  title         = {Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model},
  author        = {Lianghui Zhu and Bencheng Liao and Qian Zhang and Xinlong Wang and Wenyu Liu and Xinggang Wang},
  year          = {2024},
  eprint        = {2401.09417},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2401.09417}
}

@misc{clip,
  title         = {Learning Transferable Visual Models From Natural Language Supervision},
  author        = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  year          = {2021},
  eprint        = {2103.00020},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2103.00020}
}

@misc{denoising,
  title         = {Denoising Vision Transformers},
  author        = {Jiawei Yang and Katie Z Luo and Jiefeng Li and Congyue Deng and Leonidas Guibas and Dilip Krishnan and Kilian Q Weinberger and Yonglong Tian and Yue Wang},
  year          = {2024},
  eprint        = {2401.02957},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2401.02957}
}